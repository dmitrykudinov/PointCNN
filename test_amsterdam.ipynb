{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Testing On Segmentation Task.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import h5py\n",
    "import argparse\n",
    "import importlib\n",
    "import data_utils\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(\n",
    "    filelist = '../data/Amsterdam/test_files_utrecht2-las.txt',\n",
    "    load_ckpt = '../models/seg/pointcnn_seg_amsterdam_x4_12288_fps_2019-02-05-22-40-26_20662/ckpts/iter-25173',\n",
    "#     load_ckpt = '../models/seg/pointcnn_seg_amsterdam_x4_12288_fps_2019-02-10-21-10-01_17799/ckpts/iter-50346',\n",
    "    max_point_num = 24576,\n",
    "    repeat_num = 1,\n",
    "    model = 'pointcnn_seg',\n",
    "    setting = 'amsterdam_x4_12288_fps',\n",
    "    save_ply = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--filelist', '-t', help='Path to input .h5 filelist (.txt)', required=True)\n",
    "#     parser.add_argument('--load_ckpt', '-l', help='Path to a check point file for load', required=True)\n",
    "#     parser.add_argument('--max_point_num', '-p', help='Max point number of each sample', type=int, default=8192)\n",
    "#     parser.add_argument('--repeat_num', '-r', help='Repeat number', type=int, default=1)\n",
    "#     parser.add_argument('--model', '-m', help='Model to use', required=True)\n",
    "#     parser.add_argument('--setting', '-x', help='Setting to use', required=True)\n",
    "#     parser.add_argument('--save_ply', '-s', help='Save results as ply', action='store_true')\n",
    "#     args = parser.parse_args()\n",
    "#     print(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dkudinov/TF/vol1_1TB/MeshSegmentation/PointCNN/pointfly.py:123: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/dkudinov/.local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "2019-03-08 14:02:05.672273-Parameter number: 3278203.\n"
     ]
    }
   ],
   "source": [
    "model = importlib.import_module(args.model)\n",
    "setting_path = os.path.join(args.model)\n",
    "sys.path.append(setting_path)\n",
    "setting = importlib.import_module(args.setting)\n",
    "\n",
    "sample_num = setting.sample_num\n",
    "max_point_num = args.max_point_num\n",
    "batch_size = args.repeat_num * math.ceil(max_point_num / sample_num)\n",
    "\n",
    "######################################################################\n",
    "# Placeholders\n",
    "indices = tf.placeholder(tf.int32, shape=(batch_size, None, 2), name=\"indices\")\n",
    "is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "pts_fts = tf.placeholder(tf.float32, shape=(batch_size, max_point_num, setting.data_dim), name='points')\n",
    "######################################################################\n",
    "\n",
    "######################################################################\n",
    "pts_fts_sampled = tf.gather_nd(pts_fts, indices=indices, name='pts_fts_sampled')\n",
    "if setting.data_dim > 3:\n",
    "    points_sampled, features_sampled = tf.split(pts_fts_sampled,\n",
    "                                                [3, setting.data_dim - 3],\n",
    "                                                axis=-1,\n",
    "                                                name='split_points_features')\n",
    "    if not setting.use_extra_features:\n",
    "        features_sampled = None\n",
    "else:\n",
    "    points_sampled = pts_fts_sampled\n",
    "    features_sampled = None\n",
    "\n",
    "net = model.Net(points_sampled, features_sampled, is_training, setting)\n",
    "seg_probs_op = tf.nn.softmax(net.logits, name='seg_probs')\n",
    "\n",
    "# for restore model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "parameter_num = np.sum([np.prod(v.shape.as_list()) for v in tf.trainable_variables()])\n",
    "print('{}-Parameter number: {:d}.'.format(datetime.now(), parameter_num))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time_dict = {}\n",
    "total_time_dict = {}\n",
    "\n",
    "current_time_ms = lambda:int(round(time.time()*1000))\n",
    "\n",
    "def timer_start(msg): \n",
    "    global start_time_dict\n",
    "    global total_time_dict\n",
    "    start_time_dict[msg] = current_time_ms()\n",
    "    if not msg in total_time_dict:\n",
    "        total_time_dict[msg] = 0\n",
    "    \n",
    "def timer_pause(msg):\n",
    "    global start_time_dict\n",
    "    global total_time_dict\n",
    "    total_time_dict[msg] += current_time_ms() - start_time_dict[msg]\n",
    "    \n",
    "def timer_stop(msg):\n",
    "    global total_time_dict\n",
    "    timer_pause(msg)\n",
    "    print(\"{} completed in {}ms\".format(msg, total_time_dict[msg]))\n",
    "    total_time_dict[msg] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/seg/pointcnn_seg_amsterdam_x4_12288_fps_2019-02-05-22-40-26_20662/ckpts/iter-25173\n",
      "2019-03-08 14:02:09.260587-Checkpoint loaded from ../models/seg/pointcnn_seg_amsterdam_x4_12288_fps_2019-02-05-22-40-26_20662/ckpts/iter-25173!\n",
      "Reading ../data/Amsterdam/./tmp/utrecht_las_00107_zero_0.h5... completed in 1ms\n",
      "2019-03-08 14:02:09.503222-385 testing batches.\n",
      "2019-03-08 14:02:40.138440-Processing 100 of 385 batches.\n",
      "2019-03-08 14:03:07.840315-Processing 200 of 385 batches.\n",
      "2019-03-08 14:03:34.685784-Processing 300 of 385 batches.\n",
      "2019-03-08 14:03:57.222791-Processing 384 of 385 batches.\n",
      "Preprocessing completed in 547ms\n",
      "Inference completed in 35853ms\n",
      "Postprocessing completed in 72219ms\n",
      "Saving ../data/Amsterdam/./tmp/utrecht_las_00107_zero_0_pred.h5... completed in 122ms\n",
      "Reading ../data/Amsterdam/./tmp/utrecht_las_00107_half_0.h5... completed in 0ms\n",
      "2019-03-08 14:03:57.904224-395 testing batches.\n",
      "2019-03-08 14:04:25.874746-Processing 100 of 395 batches.\n",
      "2019-03-08 14:04:52.532714-Processing 200 of 395 batches.\n",
      "2019-03-08 14:05:18.956505-Processing 300 of 395 batches.\n",
      "2019-03-08 14:05:43.893545-Processing 394 of 395 batches.\n",
      "Preprocessing completed in 656ms\n",
      "Inference completed in 34986ms\n",
      "Postprocessing completed in 71590ms\n",
      "Saving ../data/Amsterdam/./tmp/utrecht_las_00107_half_0_pred.h5... completed in 121ms\n",
      "2019-03-08 14:05:44.349723-Done!\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    # Load the model\n",
    "    saver.restore(sess, args.load_ckpt)\n",
    "    print('{}-Checkpoint loaded from {}!'.format(datetime.now(), args.load_ckpt))\n",
    "\n",
    "    indices_batch_indices = np.tile(np.reshape(np.arange(batch_size), (batch_size, 1, 1)), (1, sample_num, 1))\n",
    "\n",
    "    folder = os.path.dirname(args.filelist)\n",
    "    filenames = [os.path.join(folder, line.strip()) for line in open(args.filelist)]\n",
    "    for filename in filenames:\n",
    "        msg = 'Reading {}...'.format(filename)\n",
    "        timer_start(msg)\n",
    "        data_h5 = h5py.File(filename)\n",
    "        timer_stop(msg)\n",
    "        \n",
    "        data = data_h5['data'][...].astype(np.float32)\n",
    "        data_num = data_h5['data_num'][...].astype(np.int32)\n",
    "        batch_num = data.shape[0]\n",
    "\n",
    "        labels_pred = np.full((batch_num, max_point_num), -1, dtype=np.int32)\n",
    "        confidences_pred = np.zeros((batch_num, max_point_num), dtype=np.float32)\n",
    "\n",
    "        print('{}-{:d} testing batches.'.format(datetime.now(), batch_num))\n",
    "        for batch_idx in range(batch_num):\n",
    "            if batch_idx % 100 == 0 and batch_idx > 0 or batch_idx == batch_num-1:\n",
    "                print('{}-Processing {} of {} batches.'.format(datetime.now(), batch_idx, batch_num))\n",
    "                \n",
    "            msg = \"Preprocessing\"\n",
    "            timer_start(msg)\n",
    "            points_batch = data[[batch_idx] * batch_size, ...]\n",
    "            point_num = data_num[batch_idx]\n",
    "\n",
    "            tile_num = math.ceil((sample_num * batch_size) / point_num)\n",
    "            indices_shuffle = np.tile(np.arange(point_num), tile_num)[0:sample_num * batch_size]\n",
    "            np.random.shuffle(indices_shuffle)\n",
    "            indices_batch_shuffle = np.reshape(indices_shuffle, (batch_size, sample_num, 1))\n",
    "            indices_batch = np.concatenate((indices_batch_indices, indices_batch_shuffle), axis=2)\n",
    "            timer_pause(msg)\n",
    "\n",
    "            msg = \"Inference\"\n",
    "            timer_start(msg)\n",
    "            seg_probs = sess.run([seg_probs_op],\n",
    "                                    feed_dict={\n",
    "                                        pts_fts: points_batch,\n",
    "                                        indices: indices_batch,\n",
    "                                        is_training: False,\n",
    "                                    })\n",
    "            timer_pause(msg)\n",
    "            \n",
    "            msg = \"Postprocessing\"\n",
    "            timer_start(msg)\n",
    "            probs_2d = np.reshape(seg_probs, (sample_num * batch_size, -1))\n",
    "\n",
    "            predictions = [(-1, 0.0)] * point_num\n",
    "            for idx in range(sample_num * batch_size):\n",
    "                point_idx = indices_shuffle[idx]\n",
    "                probs = probs_2d[idx, :]\n",
    "                confidence = np.amax(probs)\n",
    "                label = np.argmax(probs)\n",
    "                if confidence > predictions[point_idx][1]:\n",
    "                    predictions[point_idx] = [label, confidence]\n",
    "            labels_pred[batch_idx, 0:point_num] = np.array([label for label, _ in predictions])\n",
    "            confidences_pred[batch_idx, 0:point_num] = np.array([confidence for _, confidence in predictions])\n",
    "            timer_pause(msg)            \n",
    "\n",
    "        timer_stop(\"Preprocessing\")\n",
    "        timer_stop(\"Inference\")\n",
    "        timer_stop(\"Postprocessing\")\n",
    "            \n",
    "        filename_pred = filename[:-3] + '_pred.h5'\n",
    "        msg = 'Saving {}...'.format(filename_pred)\n",
    "        timer_start(msg)\n",
    "        file = h5py.File(filename_pred, 'w')\n",
    "        file.create_dataset('data_num', data=data_num)\n",
    "        file.create_dataset('label_seg', data=labels_pred)\n",
    "        file.create_dataset('confidence', data=confidences_pred)\n",
    "        has_indices = 'indices_split_to_full' in data_h5\n",
    "        if has_indices:\n",
    "            file.create_dataset('indices_split_to_full', data=data_h5['indices_split_to_full'][...])\n",
    "        file.close()\n",
    "        timer_stop(msg)\n",
    "\n",
    "        if args.save_ply:\n",
    "            msg = 'Saving ply of {}...'.format(filename_pred)\n",
    "            timer_start(msg)\n",
    "            filepath_label_ply = os.path.join(filename_pred[:-3] + 'ply_label')\n",
    "            data_utils.save_ply_property_batch(data[:, :, 0:3], labels_pred[...],\n",
    "                                               filepath_label_ply, data_num[...], setting.num_class)\n",
    "            timer_stop(msg)\n",
    "        ######################################################################\n",
    "        \n",
    "    print('{}-Done!'.format(datetime.now()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
